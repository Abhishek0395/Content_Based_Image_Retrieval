{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'C:\\\\Ana_conda\\\\envs\\\\workshop\\\\python35.zip', 'C:\\\\Ana_conda\\\\envs\\\\workshop\\\\DLLs', 'C:\\\\Ana_conda\\\\envs\\\\workshop\\\\lib', 'C:\\\\Ana_conda\\\\envs\\\\workshop', 'C:\\\\Ana_conda\\\\envs\\\\workshop\\\\lib\\\\site-packages', 'C:\\\\Ana_conda\\\\envs\\\\workshop\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Ana_conda\\\\envs\\\\workshop\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Ana_conda\\\\envs\\\\workshop\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Ana_conda\\\\envs\\\\workshop\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\HP\\\\.ipython']\n",
      "Collecting scikit-image\n",
      "  Downloading https://files.pythonhosted.org/packages/66/49/e01edd0c8baab1b24ec6b9f25b3cbb6f6d400d522de44899cb9789abeb65/scikit_image-0.14.2-cp35-none-win_amd64.whl (24.6MB)\n",
      "Collecting dask[array]>=1.0.0 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/96/bce48a61f6d84f5db6dc4febb4be96009c95d6bdfc67e08cc55f644ae36b/dask-1.1.0-py2.py3-none-any.whl (699kB)\n",
      "Collecting PyWavelets>=0.4.0 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/16/d9/e76ead8b4640d6e29b82d7d034abe44830a0501d84ba4c7c7bf955be09ef/PyWavelets-1.0.1-cp35-none-win_amd64.whl (4.2MB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\ana_conda\\envs\\workshop\\lib\\site-packages (from scikit-image) (1.12.0)\n",
      "Collecting pillow>=4.3.0 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/ba/43f2f2dd60f304d8563af82ecd4822ff0b57ddfd71631c407fce69da84d1/Pillow-5.4.1-cp35-cp35m-win_amd64.whl (1.9MB)\n",
      "Collecting networkx>=1.8 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/f3/f4/7e20ef40b118478191cec0b58c3192f822cace858c19505c7670961b76b2/networkx-2.2.zip (1.7MB)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in c:\\ana_conda\\envs\\workshop\\lib\\site-packages (from scikit-image) (0.5.5)\n",
      "Collecting toolz>=0.7.3; extra == \"array\" (from dask[array]>=1.0.0->scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/d0/a73c15bbeda3d2e7b381a36afb0d9cd770a9f4adc5d1532691013ba881db/toolz-0.9.0.tar.gz (45kB)\n",
      "Requirement already satisfied: numpy>=1.11.0; extra == \"array\" in c:\\ana_conda\\envs\\workshop\\lib\\site-packages (from dask[array]>=1.0.0->scikit-image) (1.16.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\ana_conda\\envs\\workshop\\lib\\site-packages (from networkx>=1.8->scikit-image) (4.3.0)\n",
      "Building wheels for collected packages: networkx, toolz\n",
      "  Building wheel for networkx (setup.py): started\n",
      "  Building wheel for networkx (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\HP\\AppData\\Local\\pip\\Cache\\wheels\\68\\f8\\29\\b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91\n",
      "  Building wheel for toolz (setup.py): started\n",
      "  Building wheel for toolz (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\HP\\AppData\\Local\\pip\\Cache\\wheels\\f4\\0c\\f6\\ce6b2d1aa459ee97cc3c0f82236302bd62d89c86c700219463\n",
      "Successfully built networkx toolz\n",
      "Installing collected packages: toolz, dask, PyWavelets, pillow, networkx, scikit-image\n",
      "Successfully installed PyWavelets-1.0.1 dask-1.1.0 networkx-2.2 pillow-5.4.1 scikit-image-0.14.2 toolz-0.9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#print(os.sys.path)\n",
    "import cv2\n",
    "#!pip install scikit-image\n",
    "from skimage.feature import local_binary_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catagory_0\n",
      "catagory_1\n",
      "catagory_2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory_list = list()\n",
    "dir_path = \"dataset\"\n",
    "for root, dirs, files in os.walk(dir_path, topdown=False):\n",
    "    for name in dirs:\n",
    "        directory_list.append((name))\n",
    "#print(directory_list)\n",
    "for directory in directory_list:\n",
    "    print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "labels = 'labels.csv'\n",
    "labelfile = open(labels,'w')\n",
    "for i in range(len(directory_list)):\n",
    "    readpath = (dir_path+'/' + directory_list[i]+'/*jpg')\n",
    "    #print(readpath)\n",
    "    images = glob.glob(readpath)\n",
    "    for image in images:\n",
    "        labelfile.write(image+','+directory_list[i][9:len(directory_list[i])]+'\\n')\n",
    "labelfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "labels = 'labels.csv'\n",
    "shuffled_labels = 'Shuffled_labels.csv'\n",
    "\n",
    "labelfile = open(labels, \"r\")\n",
    "lines = labelfile.readlines()\n",
    "labelfile.close()\n",
    "random.shuffle(lines)\n",
    "\n",
    "shufflefile = open(shuffled_labels, \"w\")\n",
    "shufflefile.writelines(lines)\n",
    "shufflefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "#to check uniform or not \n",
    "def uniform(pattern):\n",
    "    pat= int(pattern)\n",
    "    a=0\n",
    "    b=0\n",
    "    cnt=0\n",
    "    for i in range(0,8):\n",
    "        if( i==0 ):\n",
    "            a= int(pattern/2**(7-i))\n",
    "        else:\n",
    "            b= int(pattern/2**(7-i))\n",
    "            if(b!=a):\n",
    "                cnt=cnt+1\n",
    "                a=b\n",
    "        pattern=pattern%2**(7-i)\n",
    "    if(cnt<=2):\n",
    "        return 1 #uniform hbe\n",
    "    else:\n",
    "        return 0\n",
    "items = []\n",
    "true_items = []\n",
    "for i in range(0, 256):\n",
    "    if uniform(i):\n",
    "        items.append(i)\n",
    "        true_items.append(i)\n",
    "        \n",
    "items.append(250)\n",
    "true_items.append(250)\n",
    "print(len(items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def get(image, idx, idy):\n",
    "    if idx < (len(image)) and idy < len(image[0]) and idx>=0 and idy >=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def dir(x,y,img):\n",
    "\tax=int(img[x+1,y])-int(img[x,y])\n",
    "\tay=int(img[x,y-1])-int(img[x,y])\n",
    "    \n",
    "\tif ax>=0 and ay>=0:\n",
    "\t\treturn 1\n",
    "\telif ax<0 and ay>=0:\n",
    "\t\treturn 2\n",
    "\telif ax<0 and ay<0:\n",
    "\t\treturn 3\n",
    "\telif ax>=0 and ay<0:\n",
    "\t\treturn 4\n",
    "\t\t\n",
    "def d_lbp(x,y,img):\n",
    "    out=[]\n",
    "    fx=[0,-1,-1,-1,0,1,1,1]\n",
    "    fy=[1,1,0,-1,-1,-1,0,1]\n",
    "    val = img[x][y]\n",
    "    for i in range(0, 8):\n",
    "        n_x = x+fx[i]\n",
    "        n_y = y+fy[i]\n",
    "        val1=img[n_x][n_y]\n",
    "        if val1>=val:\n",
    "            out.append(1)\n",
    "        else:\n",
    "            out.append(0)\n",
    "    #print(out)\n",
    "    return out\n",
    "\n",
    "def add_dict(dic, val):\n",
    "    if val not in dic.keys():\n",
    "        dic[250] = dic[250] + 1\n",
    "    else:\n",
    "        dic[val] = dic[val] + 1\n",
    "\n",
    "def lbp(img, dic1):\n",
    "    fx=[0,-1,-1,-1,0,1,1,1]\n",
    "    fy=[1,1,0,-1,-1,-1,0,1]\n",
    "    for x in range(1, len(img)-1):\n",
    "        for y in range(1, len(img[x])-1):\n",
    "            value = d_lbp(x,y,img)\n",
    "            val1=0\n",
    "            for i in range(len(value)):\n",
    "                if(value[i]==1):\n",
    "                    val1+=2**(7-i)\n",
    "            add_dict(dic1, val1)\n",
    "    return dic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dataset/catagory_1\\D1_300_3.jpg  label = 1 done in 0.40576815605163574\n",
      "100 dataset/catagory_3\\D3_30_8.jpg  label = 3 done in 4.2475745677948\n",
      "200 dataset/catagory_0\\D112_270_25.jpg  label = 0 done in 4.5064311027526855\n",
      "300 dataset/catagory_2\\D2_90_5.jpg  label = 2 done in 4.33467698097229\n",
      "400 dataset/catagory_6\\D6_0_17.jpg  label = 6 done in 3.9850940704345703\n",
      "500 dataset/catagory_5\\D5_180_5.jpg  label = 5 done in 3.5910046100616455\n",
      "600 dataset/catagory_1\\D1_0_17.jpg  label = 1 done in 3.6768603324890137\n",
      "700 dataset/catagory_4\\D4_0_15.jpg  label = 4 done in 3.456049919128418\n",
      "800 dataset/catagory_6\\D6_0_7.jpg  label = 6 done in 3.0632503032684326\n",
      "900 dataset/catagory_1\\D1_300_6.jpg  label = 1 done in 3.558971881866455\n",
      "1000 dataset/catagory_0\\D112_30_9.jpg  label = 0 done in 3.351088047027588\n",
      "1100 dataset/catagory_5\\D5_270_23.jpg  label = 5 done in 3.3381214141845703\n",
      "1200 dataset/catagory_2\\D2_30_5.jpg  label = 2 done in 3.400059461593628\n",
      "1300 dataset/catagory_4\\D4_270_8.jpg  label = 4 done in 3.3910651206970215\n",
      "1400 dataset/catagory_7\\D7_300_11.jpg  label = 7 done in 3.508995771408081\n",
      "1500 dataset/catagory_1\\D1_0_4.jpg  label = 1 done in 3.183183193206787\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from scipy.stats import itemfreq\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "df = pd.read_csv(shuffled_labels, header=None)\n",
    "rows = df.iterrows()\n",
    "\n",
    "X_addrs = []\n",
    "X_hist = []\n",
    "Y_hist = []\n",
    "#row = rows[0]\n",
    "j = 0\n",
    "\n",
    "# uncomment below this for local tetra pattern\n",
    "dic1 = {}\n",
    "\n",
    "\n",
    "def create_dic():\n",
    "    for i in range(len(items)):\n",
    "        dic1[items[i]] = 0\n",
    "\n",
    "start_time = time.time()   \n",
    "'''\n",
    "for row in rows:\n",
    "    #print(row[1][1])\n",
    "    create_dic()\n",
    "    img = cv2.imread(row[1][0])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    new_x = []\n",
    "    dic1 = lbp(img,dic1)\n",
    "    \n",
    "    x1 = []\n",
    "    for i in dic1.keys():\n",
    "        x1.append(dic1[i])\n",
    "    x1 = np.array(x1)\n",
    "    #print(x1)\n",
    "    x1 = x1/np.sum(x1)\n",
    "    \n",
    "    new_x = x1.tolist()\n",
    "    \n",
    "    X_hist.append(new_x)\n",
    "    X_addrs.append(row[1][0])\n",
    "    Y_hist.append(row[1][1])\n",
    "    if j%100 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(str(j) +\" \"+str(row[1][0])+\"  label = \"+str(row[1][1]) + \" done in \" + str(elapsed_time))\n",
    "        start_time = time.time()\n",
    "    j = j + 1\n",
    "    \n",
    "'''\n",
    "for row in rows:\n",
    "    #print(row[1][1])\n",
    "    im = cv2.imread(row[1][0])\n",
    "    #print(im.shape)\n",
    "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    radius = 1\n",
    "    # Number of points to be considered as neighbourers \n",
    "    no_points = 8 * (radius)\n",
    "    lbp = local_binary_pattern(im_gray, no_points, radius, method='nri_uniform')\n",
    "    # Calculate the histogram\n",
    "    x = itemfreq(lbp.ravel())\n",
    "    #print(len(x))\n",
    "    #feature_length = no_points+2\n",
    "    feature_length = 59\n",
    "    dic = {}\n",
    "    for i in range(len(x)):\n",
    "        dic[x[i][0]] = x[i][1]\n",
    "    for i in range(feature_length):\n",
    "        if i not in dic.keys():\n",
    "            dic[i] = 0\n",
    "    x = [[0]*2]*feature_length\n",
    "    for i in range(feature_length):\n",
    "        x[i][0] = i\n",
    "        x[i][1] = dic[i]\n",
    "    # Normalize the histogram\n",
    "    #print(len(x))\n",
    "    if(len(x) != feature_length):\n",
    "        print(x)\n",
    "    #x = np.array(x)\n",
    "    new_x = []\n",
    "    for i in range(feature_length):\n",
    "        new_x.append(dic[i])\n",
    "    new_x = np.array(new_x)\n",
    "    hist = new_x/np.sum(new_x)\n",
    "    #hist = x[:, 1]/sum(x[:, 1])\n",
    "    # Append image path in X_name\n",
    "    X_addrs.append(row[1][0])\n",
    "    # Append histogram to X_name\n",
    "    X_hist.append(hist)\n",
    "    # Append class label in y_test\n",
    "    Y_hist.append(row[1][1])\n",
    "    if(j%100==0):\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(str(j) +\" \"+str(row[1][0])+\"  label = \"+str(row[1][1]) + \" done in \" + str(elapsed_time))\n",
    "        start_time = time.time()\n",
    "    j = j + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/catagory_1\\D1_300_3.jpg\n",
      "[ 0.08319092  0.019104    0.00274658  0.01934814  0.00311279  0.0178833\n",
      "  0.00183105  0.02130127  0.00354004  0.012146    0.00701904  0.01123047\n",
      "  0.00720215  0.01000977  0.00628662  0.01019287  0.00738525  0.02026367\n",
      "  0.00720215  0.0177002   0.00921631  0.01641846  0.00671387  0.01635742\n",
      "  0.00909424  0.00994873  0.02270508  0.01025391  0.02508545  0.00854492\n",
      "  0.01928711  0.00732422  0.02380371  0.01538086  0.01641846  0.01434326\n",
      "  0.01318359  0.01593018  0.01397705  0.01251221  0.01287842  0.01287842\n",
      "  0.00811768  0.01220703  0.00860596  0.01263428  0.00842285  0.01177979\n",
      "  0.00872803  0.00384521  0.01971436  0.00634766  0.0144043   0.00445557\n",
      "  0.01727295  0.00531006  0.01544189  0.08728027  0.15447998]\n",
      "1\n",
      "1504\n",
      "59\n",
      "(1504, 59)\n"
     ]
    }
   ],
   "source": [
    "print(X_addrs[0])\n",
    "print(X_hist[0])\n",
    "print(Y_hist[0])\n",
    "#print(X_hist)\n",
    "X_hist = np.array(X_hist)\n",
    "Y_hist = np.array(Y_hist)\n",
    "print(len(X_addrs))\n",
    "print(len(X_hist[50]))\n",
    "print(X_hist.shape)\n",
    "#print((Y_hist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1504, 59) (1504,)\n",
      "1052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(X_train)):\\n    if(len(X_train[i]) != 26):\\n       print(len(X_train[i]))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hist = np.array(X_hist)\n",
    "Y_hist = np.array(Y_hist)\n",
    "\n",
    "print(X_hist.shape, Y_hist.shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_hist), np.array(Y_hist), test_size=0.3)\n",
    "X_train = np.array(X_train)\n",
    "print(len(X_train))\n",
    "\n",
    "'''\n",
    "for i in range(len(X_train)):\n",
    "    if(len(X_train[i]) != 26):\n",
    "       print(len(X_train[i]))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1504, 59)\n",
      "0.7333333333333333\n",
      "0.8166666666666667\n",
      "0.8166666666666667\n"
     ]
    }
   ],
   "source": [
    "#this code is to use the distance function\n",
    "def feature_distance(feature1, feature2):\n",
    "    dist = 0.0\n",
    "    for i in range(len(feature1)):\n",
    "        dist += abs((feature1[i]*1.0-feature2[i]*1.0)/(1.0+feature1[i]*1.0+feature2[i]*1.0))\n",
    "        #print(dist)\n",
    "    return dist\n",
    "x = X_hist\n",
    "print(x.shape)\n",
    "#print(unique_name[X_addrs[0]])\n",
    "#print(Y_hist[0])\n",
    "#print(Y_hist[1])\n",
    "#print(feature_distance(x[0], x[10]))\n",
    "\n",
    "#60 -> 0.\n",
    "#50->0.\n",
    "#40->0.\n",
    "#35->0.\n",
    "#30->0.\n",
    "\n",
    "sm = 0\n",
    "def clc():\n",
    "    query_length = 60\n",
    "    for j in range(3):\n",
    "        true_val = 0\n",
    "        false_val = 0\n",
    "        distance_list = []\n",
    "        query = x[j]\n",
    "        query_index = j\n",
    "        for i in range(len(Y_hist)):\n",
    "            distance_list.append(feature_distance(query, x[i]))\n",
    "        unsorted = zip(distance_list, Y_hist)\n",
    "        sorted_touple = sorted(unsorted, key = lambda element : element[0])\n",
    "        #print(len(sorted_touple))\n",
    "        for i in range(query_length):\n",
    "            if(sorted_touple[i][1] == Y_hist[query_index]):\n",
    "                true_val = true_val + 1\n",
    "            else:\n",
    "                #print(\"this is wrong \"+str(i)+\" confusing with \"+str(sorted_touple[i][1]))\n",
    "                false_val = false_val+1\n",
    "        print(true_val*1.0/query_length*1.0)\n",
    "clc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#clf = svm.SVC()\n",
    "#clf = GaussianNB()\n",
    "#clf = LogisticRegression()\n",
    "#clf = MLPClassifier(alpha=1)\n",
    "#clf = AdaBoostClassifier()\n",
    "clf = RandomForestClassifier()\n",
    "#clf = DecisionTreeClassifier()\n",
    "\n",
    "#X_train = np.array(X_train)\n",
    "print(type(X_train))\n",
    "\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#clf = pickle.load(open('RandomForest_model.sav', 'rb'))\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train,Y_train)\n",
    "scores = cross_val_score(clf,X_test,Y_test,cv=5)\n",
    "print((scores))\n",
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#for saving model\n",
    "filename = 'DecisionTree_model.sav'\n",
    "#pickle.dump(clf, open(filename, 'wb'))\n",
    "clf = pickle.load(open('DecisionTree_model.sav', 'rb'))\n",
    "clf.score(X_test, Y_test)\n",
    "#scores = cross_val_score(clf, X_test, Y_test, cv=20)\n",
    "#print((scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "y = np.array([1, 1, 2, 2])\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "print(type(y))\n",
    "clf.fit(X, y) \n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [0]*26\n",
    "a = np.array(a)\n",
    "b = a.copy()\n",
    "b = [0]*14\n",
    "b = np.array(b)\n",
    "print(b.shape)\n",
    "c = a+b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lbp-> Local derivative ternary lbp last e  tetra "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
