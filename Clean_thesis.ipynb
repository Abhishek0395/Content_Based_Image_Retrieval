{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catagory_0\n",
      "catagory_1\n",
      "catagory_10\n",
      "catagory_100\n",
      "catagory_101\n",
      "catagory_102\n",
      "catagory_103\n",
      "catagory_104\n",
      "catagory_105\n",
      "catagory_106\n",
      "catagory_107\n",
      "catagory_108\n",
      "catagory_109\n",
      "catagory_11\n",
      "catagory_110\n",
      "catagory_111\n",
      "catagory_12\n",
      "catagory_13\n",
      "catagory_14\n",
      "catagory_15\n",
      "catagory_16\n",
      "catagory_17\n",
      "catagory_18\n",
      "catagory_19\n",
      "catagory_2\n",
      "catagory_20\n",
      "catagory_21\n",
      "catagory_22\n",
      "catagory_23\n",
      "catagory_24\n",
      "catagory_25\n",
      "catagory_26\n",
      "catagory_27\n",
      "catagory_28\n",
      "catagory_29\n",
      "catagory_3\n",
      "catagory_30\n",
      "catagory_31\n",
      "catagory_32\n",
      "catagory_33\n",
      "catagory_34\n",
      "catagory_35\n",
      "catagory_36\n",
      "catagory_37\n",
      "catagory_38\n",
      "catagory_39\n",
      "catagory_4\n",
      "catagory_40\n",
      "catagory_41\n",
      "catagory_42\n",
      "catagory_43\n",
      "catagory_44\n",
      "catagory_45\n",
      "catagory_46\n",
      "catagory_47\n",
      "catagory_48\n",
      "catagory_49\n",
      "catagory_5\n",
      "catagory_50\n",
      "catagory_51\n",
      "catagory_52\n",
      "catagory_53\n",
      "catagory_54\n",
      "catagory_55\n",
      "catagory_56\n",
      "catagory_57\n",
      "catagory_58\n",
      "catagory_59\n",
      "catagory_6\n",
      "catagory_60\n",
      "catagory_61\n",
      "catagory_62\n",
      "catagory_63\n",
      "catagory_64\n",
      "catagory_65\n",
      "catagory_66\n",
      "catagory_67\n",
      "catagory_68\n",
      "catagory_69\n",
      "catagory_7\n",
      "catagory_70\n",
      "catagory_71\n",
      "catagory_72\n",
      "catagory_73\n",
      "catagory_74\n",
      "catagory_75\n",
      "catagory_76\n",
      "catagory_77\n",
      "catagory_78\n",
      "catagory_79\n",
      "catagory_8\n",
      "catagory_80\n",
      "catagory_81\n",
      "catagory_82\n",
      "catagory_83\n",
      "catagory_84\n",
      "catagory_85\n",
      "catagory_86\n",
      "catagory_87\n",
      "catagory_88\n",
      "catagory_89\n",
      "catagory_9\n",
      "catagory_90\n",
      "catagory_91\n",
      "catagory_92\n",
      "catagory_93\n",
      "catagory_94\n",
      "catagory_95\n",
      "catagory_96\n",
      "catagory_97\n",
      "catagory_98\n",
      "catagory_99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "from skimage.feature import local_binary_pattern\n",
    "import os\n",
    "import glob\n",
    "\n",
    "directory_list = list()\n",
    "dir_path = \"C:/Users/HP/Desktop/Thesis project/full_dataset\"\n",
    "for root, dirs, files in os.walk(dir_path, topdown=False):\n",
    "    for name in dirs:\n",
    "        directory_list.append((name))\n",
    "for directory in directory_list:\n",
    "    print(directory)\n",
    "\n",
    "labels = 'C:/Users/HP/Desktop/Thesis project/labels.csv'\n",
    "labelfile = open(labels,'w')\n",
    "for i in range(len(directory_list)):\n",
    "    readpath = (dir_path+'/' + directory_list[i]+'/*jpg')\n",
    "    images = glob.glob(readpath)\n",
    "    for image in images:\n",
    "        labelfile.write(image+','+str(i)+'\\n')\n",
    "labelfile.close()\n",
    "\n",
    "import random\n",
    "labels = 'C:/Users/HP/Desktop/Thesis project/labels.csv'\n",
    "shuffled_labels = 'C:/Users/HP/Desktop/Thesis project/Shuffled_labels.csv'\n",
    "\n",
    "labelfile = open(labels, \"r\")\n",
    "lines = labelfile.readlines()\n",
    "labelfile.close()\n",
    "random.shuffle(lines)\n",
    "\n",
    "shufflefile = open(shuffled_labels, \"w\")\n",
    "shufflefile.writelines(lines)\n",
    "shufflefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done \n",
      "100 done \n",
      "200 done \n",
      "300 done \n",
      "400 done \n",
      "500 done \n",
      "600 done \n",
      "700 done \n",
      "800 done \n",
      "900 done \n",
      "1000 done \n",
      "1100 done \n",
      "1200 done \n",
      "1300 done \n",
      "1400 done \n",
      "1500 done \n",
      "1600 done \n",
      "1700 done \n",
      "1800 done \n",
      "1900 done \n",
      "2000 done \n",
      "2100 done \n",
      "2200 done \n",
      "2300 done \n",
      "2400 done \n",
      "2500 done \n",
      "2600 done \n",
      "2700 done \n",
      "2800 done \n",
      "2900 done \n",
      "3000 done \n",
      "3100 done \n",
      "3200 done \n",
      "3300 done \n",
      "3400 done \n",
      "3500 done \n",
      "3600 done \n",
      "3700 done \n",
      "3800 done \n",
      "3900 done \n",
      "4000 done \n",
      "4100 done \n",
      "4200 done \n",
      "4300 done \n",
      "4400 done \n",
      "4500 done \n",
      "4600 done \n",
      "4700 done \n",
      "4800 done \n",
      "4900 done \n",
      "5000 done \n",
      "5100 done \n",
      "5200 done \n",
      "5300 done \n",
      "5400 done \n",
      "5500 done \n",
      "5600 done \n",
      "5700 done \n",
      "5800 done \n",
      "5900 done \n",
      "6000 done \n",
      "6100 done \n",
      "6200 done \n",
      "6300 done \n",
      "6400 done \n",
      "6500 done \n",
      "6600 done \n",
      "6700 done \n",
      "6800 done \n",
      "6900 done \n",
      "7000 done \n",
      "7100 done \n",
      "7200 done \n",
      "7300 done \n",
      "7400 done \n",
      "7500 done \n",
      "7600 done \n",
      "7700 done \n",
      "7800 done \n",
      "7900 done \n",
      "8000 done \n",
      "8100 done \n",
      "8200 done \n",
      "8300 done \n",
      "8400 done \n",
      "8500 done \n",
      "8600 done \n",
      "8700 done \n",
      "8800 done \n",
      "8900 done \n",
      "9000 done \n",
      "9100 done \n",
      "9200 done \n",
      "9300 done \n",
      "9400 done \n",
      "9500 done \n",
      "9600 done \n",
      "9700 done \n",
      "9800 done \n",
      "9900 done \n",
      "10000 done \n",
      "10100 done \n",
      "10200 done \n",
      "10300 done \n",
      "10400 done \n",
      "10500 done \n",
      "10600 done \n",
      "10700 done \n",
      "10800 done \n",
      "10900 done \n",
      "11000 done \n",
      "11100 done \n",
      "11200 done \n",
      "11300 done \n",
      "11400 done \n",
      "11500 done \n",
      "11600 done \n",
      "11700 done \n",
      "11800 done \n",
      "11900 done \n",
      "12000 done \n",
      "12100 done \n",
      "12200 done \n",
      "12300 done \n",
      "12400 done \n",
      "12500 done \n",
      "12600 done \n",
      "12700 done \n",
      "12800 done \n",
      "12900 done \n",
      "13000 done \n",
      "13100 done \n",
      "13200 done \n",
      "13300 done \n",
      "13400 done \n",
      "13500 done \n",
      "13600 done \n",
      "13700 done \n",
      "13800 done \n",
      "13900 done \n",
      "14000 done \n",
      "14100 done \n",
      "14200 done \n",
      "14300 done \n",
      "14400 done \n",
      "14500 done \n",
      "14600 done \n",
      "14700 done \n",
      "14800 done \n",
      "14900 done \n",
      "15000 done \n",
      "15100 done \n",
      "15200 done \n",
      "15300 done \n",
      "15400 done \n",
      "15500 done \n",
      "15600 done \n",
      "15700 done \n",
      "15800 done \n",
      "15900 done \n",
      "16000 done \n",
      "16100 done \n",
      "16200 done \n",
      "16300 done \n",
      "16400 done \n",
      "16500 done \n",
      "16600 done \n",
      "16700 done \n",
      "16800 done \n",
      "16900 done \n",
      "17000 done \n",
      "17100 done \n",
      "17200 done \n",
      "17300 done \n",
      "17400 done \n",
      "17500 done \n",
      "17600 done \n",
      "17700 done \n",
      "17800 done \n",
      "17900 done \n",
      "18000 done \n",
      "18100 done \n",
      "18200 done \n",
      "18300 done \n",
      "18400 done \n",
      "18500 done \n",
      "18600 done \n",
      "18700 done \n",
      "18800 done \n",
      "18900 done \n",
      "19000 done \n",
      "19100 done \n",
      "19200 done \n",
      "19300 done \n",
      "19400 done \n",
      "19500 done \n",
      "19600 done \n",
      "19700 done \n",
      "19800 done \n",
      "19900 done \n",
      "20000 done \n",
      "20100 done \n",
      "20200 done \n",
      "20300 done \n",
      "20400 done \n",
      "20500 done \n",
      "20600 done \n",
      "20700 done \n",
      "20800 done \n",
      "20900 done \n",
      "21000 done \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from scipy.stats import itemfreq\n",
    "\n",
    "df = pd.read_csv(shuffled_labels)\n",
    "rows = df.iterrows()\n",
    "\n",
    "X_addrs = []\n",
    "X_hist = []\n",
    "Y_hist = []\n",
    "\n",
    "j = 0\n",
    "\n",
    "for row in rows:\n",
    "    im = cv2.imread(row[1][0])\n",
    "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    radius = 3\n",
    "    no_points = 8 * radius\n",
    "    lbp = local_binary_pattern(im_gray, no_points, radius, method='uniform')\n",
    "    x = itemfreq(lbp.ravel())\n",
    "    dic = {}\n",
    "    for i in range(len(x)):\n",
    "        dic[x[i][0]] = x[i][1]\n",
    "    for i in range(26):\n",
    "        if i not in dic.keys():\n",
    "            dic[i] = 0\n",
    "    x = [[0]*2]*26\n",
    "    for i in range(26):\n",
    "        x[i][0] = i\n",
    "        x[i][1] = dic[i]\n",
    "    new_x = []\n",
    "    for i in range(26):\n",
    "        new_x.append(dic[i])\n",
    "    new_x = np.array(new_x)\n",
    "    hist = new_x/np.sum(new_x)\n",
    "    X_addrs.append(row[1][0])\n",
    "    X_hist.append(hist)\n",
    "    Y_hist.append(row[1][1])\n",
    "    if(j%100==0):\n",
    "        print(str(j)+\" done \")\n",
    "    j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14738\n",
      "14738\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92670571473800856"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_hist), np.array(Y_hist), test_size=0.3)\n",
    "X_train = np.array(X_train)\n",
    "print(len(X_train))\n",
    "for i in range(len(X_train)):\n",
    "    if(len(X_train[i]) != 26):\n",
    "       print(len(X_train[i]))\n",
    "print(len(Y_train))\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#clf = svm.SVC()\n",
    "#clf = GaussianNB()\n",
    "#clf = LogisticRegression()\n",
    "#clf = MLPClassifier(alpha=1)\n",
    "#clf = AdaBoostClassifier()\n",
    "clf = RandomForestClassifier()\n",
    "#clf = DecisionTreeClassifier()\n",
    "\n",
    "#X_train = np.array(X_train)\n",
    "print(type(X_train))\n",
    "\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "clf.score(X_test, Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926705714738\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#for saving model\n",
    "filename = 'RandomForest_model_2.sav'\n",
    "#pickle.dump(clf, open(filename, 'wb'))\n",
    "clf = pickle.load(open('RandomForest_model_2.sav', 'rb'))\n",
    "clf.score(X_test, Y_test)\n",
    "#scores = cross_val_score(clf, X_test, Y_test, cv=20)\n",
    "#print((scores))\n",
    "print(clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
